# Case Study Instructions for Students

## Overview
This repository contains materials for conducting AI agent evaluations in a Legacy System Modernization scenario. The materials are organized to provide clear separation between agent context and evaluation framework.

## Repository Structure

### Agent_Context/
**Purpose**: Materials that agents should have access to during evaluation
- `Scenario/`: Complete Legacy System Modernization scenario (Oracle-to-Azure migration for MidCorp Manufacturing)
- `Tasks/`: Six core evaluation tasks for agents to complete
- `Templates/`: Document templates and standards for agent outputs
- `Supporting_Documents/`: Company profile, organizational context, technical landscape
- `AGENT_INSTRUCTIONS.md`: **CRITICAL** - Primary prompt file that agents should start with

### Case_Study/
**Purpose**: Materials for students conducting the evaluation research
- `Evaluation_Framework/`: Scoring rubrics, comparison methodologies, research protocols
- `AI_Output_Templates/`: Structured formats for capturing and analyzing agent responses
- `STUDENT_INSTRUCTIONS.md`: This file - complete methodology guide

### Artifacts/
**Purpose**: Standardized collection point for all AI agent work products
- Organized by model name and task for systematic comparison
- Contains executive summaries, detailed analyses, and research documentation

## How to Conduct Agent Evaluations

### Step 1: Understand the Complete Framework
1. **Read this document entirely** to understand the evaluation methodology
2. **Review `Agent_Context/AGENT_INSTRUCTIONS.md`** - this is what agents will receive as their primary prompt
3. **Study the scenario context**: `Agent_Context/Scenario/Legacy_System_Modernization.md`
4. **Examine supporting documents** to understand MidCorp Manufacturing's full context
5. **Review task requirements** in `Agent_Context/Tasks/Core_AI_Evaluation_Tasks.md`

### Step 2: Set Up Your Evaluation Environment
1. **Identify your target AI models** (Claude Sonnet, GPT-5, Gemini, etc.)
2. **Review `Artifacts/README.md`** to understand output organization requirements
3. **Study evaluation criteria** in `Case_Study/Evaluation_Framework/`
4. **Prepare data collection templates** from `Case_Study/AI_Output_Templates/`

### Step 3: Conduct Agent Evaluations
**CRITICAL: Use Standardized Prompting Approach**
1. **Direct each AI model to**: `Agent_Context/AGENT_INSTRUCTIONS.md` as their starting point
2. **Provide identical context**: All models receive the same instruction file
3. **Monitor output organization**: Ensure models create proper folder structure in `Artifacts/`
4. **Document methodology variations**: Note any differences in how models approach the tasks
5. **Collect conversation logs**: Capture full interaction history for analysis

### Step 4: Monitor and Guide Agent Work
1. **Verify folder structure**: Check that agents organize outputs in `Artifacts/[Model_Name]/`
2. **Ensure research compliance**: Confirm agents research missing information rather than guessing
3. **Check deliverable completeness**: Validate all required outputs are produced
4. **Note quality variations**: Document differences in professional presentation standards

### Step 5: Analyze and Compare Results
1. **Use standardized structure**: Compare outputs across models using identical organization
2. **Apply evaluation criteria** from `Case_Study/Evaluation_Framework/`
3. **Assess research quality**: Evaluate how well models sourced external information
4. **Compare strategic thinking**: Analyze depth and integration across tasks
5. **Document findings** using templates in `Case_Study/AI_Output_Templates/`

## Critical Evaluation Guidelines

### Standardized Agent Prompting
**ESSENTIAL**: All AI models must start with identical context
- **Primary Prompt**: Direct agents to `Agent_Context/AGENT_INSTRUCTIONS.md`
- **No Additional Context**: Do not provide extra information that isn't in the instruction file
- **Consistent Environment**: Use the same prompting approach across all models
- **Professional Framing**: Present as real consulting engagement, not academic exercise

### Research Requirements Monitoring
**Key Success Factor**: Verify agents research missing information rather than guessing
- **Check Source Citations**: Confirm agents cite real sources for external data
- **Validate Recent Data**: Ensure agents use 2024-2025 information where available
- **Monitor Methodology**: Document how each model approaches research tasks
- **Quality Assessment**: Evaluate reliability and relevance of sources used

### Output Organization Verification
**Standardization Requirement**: Confirm proper deliverable structure
- **Folder Structure**: Verify agents create `Artifacts/[Model_Name]/` organization
- **File Naming**: Check adherence to naming conventions
- **Completeness**: Ensure all required deliverables are produced
- **Professional Format**: Validate executive-appropriate presentation standards

## Key Evaluation Focus Areas

### Technical Competency
- Risk assessment depth and accuracy
- Migration strategy technical soundness
- Stakeholder analysis comprehensiveness  
- Vendor evaluation methodology

### Project Management Skills
- Timeline and budget realism
- Change management understanding
- Quality assurance planning
- Integration of business considerations

### Research and Analysis Quality
- Use of authoritative external sources
- Currency and relevance of data
- Cross-referencing of critical information
- Documentation of research methodology

### Communication Effectiveness
- Clarity and structure of outputs
- Appropriate level of detail
- Actionable recommendations
- Professional presentation

## Expected Outcomes and Success Metrics

### Model Performance Comparison
**Quantitative Measures**:
- Completeness of deliverable sets across all six tasks
- Quality of research sourcing and citation practices
- Adherence to professional formatting standards
- Consistency of recommendations across integrated tasks

**Qualitative Assessments**:
- Strategic thinking depth and business acumen
- Technical accuracy and implementation feasibility
- Stakeholder consideration and change management sophistication
- Innovation in approaches and value-added insights

### Research Insights
**Primary Research Questions**:
- Which models demonstrate superior enterprise-level strategic thinking?
- How do models compare in technical analysis versus stakeholder management tasks?
- What are the practical implications for enterprise AI adoption in project management?
- Which models provide the most implementation-ready, actionable outputs?

### Deliverable Quality Standards
**Executive Presentation Readiness**:
- Suitable for Fortune 500 executive review
- Appropriate for $2.8M-$3.5M investment decisions
- Professional presentation matching consulting firm standards
- Clear ROI justification and risk mitigation strategies

## Time Management and Logistics

### Evaluation Timeline
- **Phase 1** (5 hours): Setup, framework review, and methodology preparation
- **Phase 2** (60 hours): Execute six core tasks across four models (15 hours per model)
- **Phase 3** (10 hours): Analysis, comparison, and documentation of findings
- **Total**: 75 hours over 6-week research period

### Resource Allocation
- Allow 2.5 hours per task per model for thorough evaluation
- Reserve additional time for top-performing models on extended analysis
- Document actual time spent versus projected for efficiency comparison
- Track any variations in completion time across models

## Final Output Requirements

### Student Research Documentation
**Required Deliverables**:
- Comparative analysis report using `Case_Study/AI_Output_Templates/`
- Model performance ranking with supporting rationale
- Practical recommendations for enterprise AI adoption
- Methodology documentation and lessons learned

### Academic Standards
**Compliance Requirements**:
- Maintain academic integrity throughout evaluation process
- Document all interactions and methodology variations
- Provide objective assessment based on established criteria
- Support conclusions with specific evidence from agent outputs

---
*This evaluation framework supports systematic research on AI model capabilities in complex enterprise project management contexts. The Legacy System Modernization scenario provides authentic business complexity requiring sophisticated analytical and strategic thinking from AI agents.*